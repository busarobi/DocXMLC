%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2016 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2016,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}


\usepackage{amsthm} %proof and some other environments
\usepackage{latexsym,amsmath,amsfonts,amssymb} 


% For algorithms
\usepackage{algorithm}
%\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}
%\usepackage{cleveref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2016} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2018}


\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[subtle,mathspacing=normal, tracking=normal]{savetrees}
\usepackage{microtype}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% For theo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{Thm}{Theorem}%[section]
\newtheorem{Def}[Thm]{Definition}
\newtheorem{Rem}[Thm]{Remark}
\newtheorem{Prop}[Thm]{Proposition}
\newtheorem{Clm}[Thm]{Claim}
\newtheorem{Lem}[Thm]{Lemma}
\newtheorem{Cor}[Thm]{Corollary}
\newtheorem{Exa}[Thm]{Example}

\newenvironment{prfsk}[0]{{\noindent \it Proof sketch:}}{\hfill $\Box$\vskip10pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{mlc_ltc}

\usepackage[noend]{algpseudocode}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{booktabs} 
\usepackage{graphicx}

\newcommand{\cD}{\mathcal{D}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}

\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bkappa}{\boldsymbol{\kappa}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bW}{\mathbf{W}}


\newcommand{\bP}{\mathbf{P}}
\newcommand{\bF}{\mathbf{F}}
%\newcommand{\bY}{\mathbf{Y}}

\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bc}{\mathbf{c}}

\newcommand{\bp}{\mathbf{p}}
\newcommand{\calW}{\mathcal{W}}
%\newcommand{\bx}{\mathbf{x}}
%\newcommand{\by}{\mathbf{y}}



\newcommand*\BitAnd{\mathrel{\&}}
\newcommand*\ShiftLeft{\ll}
%\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}


\newcommand{\bigO}{\mathcal{O}}


\newcommand{\prob}{\mathbf{P}} 
\newcommand{\exptd}{\mathbf{E}}
\newcommand{\var}{\mathbf{V}} 

\newcommand{\dd}{{\, \mathrm{d}}}

\newcommand\etal{et al.}
\newcommand{\Algo}[1]{\textsc{#1}}


\newcommand\R{\mathbb{R}}   % for the real numbers
\newcommand\N{\mathbb{N}}   % for the natural numbers



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% spaces
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\figureWidthTwo}{0.6\textwidth}

\newcommand{\figureBetween}{0pt}
\newcommand{\figureBetweenHorizontal}{0pt}

\newcommand{\sectionBefore}{-0pt}
\newcommand{\sectionAfter}{-0pt}

\newcommand{\tableBefore}{-0pt}
\newcommand{\tableAfter}{-0pt}

\newcommand{\figureBefore}{-0pt}
\newcommand{\figureAfter}{-0pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\figures}{./Figs}




% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Probabilistic label trees: No-regret generalization of hierarchical softmax}

\begin{document} 

\twocolumn[
\icmltitle{Probabilistic label trees: No-regret generalization of hierarchical softmax to extreme multi-label classification}

\vskip 0.3in
]

\begin{abstract} 
Probabilistic label trees (PLTs) have been recently introduced for solving extreme multi-label classification (XMLC) problems, i.e., multi-label problems with a very large number of labels. It has already been shown that the basic variant of PLTs, based on sparse input vectors, is competitive to the state of the art approaches to XMLC, such as FastXML. In this paper, we theoretically show that PLTs are no-regret generalization of hierarchical softmax (HSM) to multi-label setting. HSM is commonly used in deep networks to deal with a large number of labels in the multi-class setting, mainly in natural language processing problems. We will also show that an existing adaptation of HSM, used for example in FastText, is not consistent. Finally, we use PLT in three different deep networks architectures for text classification, namely feature-embeddings (FastText-like representation), recurrent neural networks, and LSTMs, and show outstanding empirical results of them.
\end{abstract} 

\section{Introduction}
\label{sec:formal}

\begin{itemize}
\item PLT, 
\item proper generalization of HSM, 
\item consistency of PLT, 
\item inconsistency of FastText approximation, 
\item correction of probability estimates, 
\item PLT with sparse input, 
\item PLT with FastText embeddings, 
\item PLT with RNN, 
\item PLT with LSTM.
\end{itemize}

Extreme multi-label classification ...

In this paper we discuss the problem of document tagging or text classification ...

PLT ... 

In a nutshell, \Algo{PLT}s are based on the label tree approach~\cite{Beygelzimer_et_al_2009a,Bengio_et_al_2010,Deng_et_al_2011}, in which each leaf node corresponds to one label. Classification of a test example relies on a sequence of decisions made by node classifiers, leading the test example from the root to the leaves of the tree. Since \Algo{PLT}s are designed for multi-label classification, each internal node classifier decides whether or not to continue the path by moving to the child nodes. This is different from typical left/right decisions made in tree-based classifiers.  Moreover, a leaf node classifier needs to make a final decision regarding the prediction of a label associated to this leaf. \Algo{PLT}s use a class probability estimator in each node of the tree, such that an estimate of the posterior probability of a label associated with a leaf is given by the product of the probability estimates on the path from the root to that leaf. Prediction then relies on traversing the tree from the root to the leaf nodes. Whenever the intermediate value of the product of probabilities in an internal node is less than a given threshold, the subtree below this node is not explored anymore. This pruning strategy leads to a very efficient classification procedure. %We introduce \Algo{PLT} in more detail below. 

PLT vs. HSM

\Algo{PLT}s share similarities with conditional probability estimation trees~\cite{Beygelzimer_et_al_2009b} and probabilistic classifier chains~\cite{Dembczynski_et_al_2010c}, while being suited to estimate marginal posterior probabilities $\eta(\bx,j)$. They are also similar to Homer~\cite{Tsoumakas_et_al_2008}, which transforms training examples in the same way but does not admit a probabilistic interpretation. Let us also remark that a similar concept is known in neural networks and natural language processing under the name of hierarchical softmax~classifiers \cite{Morin_Bengio_2005}; however, it has not been used in a multi-label scenario.

We will show that PLTs are no-regret generalization of HSM to multi-label classification. 



\vspace{\sectionBefore}
\section{Formal setup of document tagging}
\label{sec:formal}
\vspace{\sectionAfter}


Input is given in form of document and set of tags pairs. Documents consist of words that are taken from a dictionary which is a finite indexed set $\calW = \{ w_1, \dots , w_N \}$. A document is a sequence of words  $\bd_i = (w_{i,1}, \dots, w_{i,n_i})\in \calW^{n_i}$. Its tag set is represented  by a binary vector $\by_i = (y_{i,1}, \ldots, y_{i,m}) \in \{ 0 , 1 \}^m$ where $m$ is the number of possible tags. A set of documents is denoted by $\cD = \{ (\bw_i, \by_{i}) \}_{i=1}^n$. For sake of simplicity, we assume that $n_i = n$ for every $1\le i \le n$.

Our approach consists of two modules: tagging and text representation modules. The text representation module $f : \calW^{n} \rightarrow \R^{d \times n+1}$ is a (not necessarily) parametric function whose input is a document and its output is the vector presentation of both words and docs. The dimension of representation space is $d$. In the simplest case, this text representation module can be the bag-of-words presentation, in which case $d=N$, and the doc vector (n+1\/th column of the image space of function $f$) is an all zero vector. A more sophisticated text presentation module is the word2vec methodology which assign a vector to each word whose dimension is typically $\le300$, and the doc presentation is empty. 

The tagging module's input is the output of the text presentation module, that is $g: \R^{d \times n+1} \rightarrow  \{ 0 , 1 \}^m$. 

In case of Deep PLT, the text representation module simply assigns a vector $\bx_{i_j} \in \R^d$ to each word of the given document $\bd_i$ where $i_j$ is the index of $j$\/th word of the document $i$, i.e. $f(\bd; \bx_1, \dots, \bx_N) = f(\bd) = \left[\bx_{i_1}, \dots , \bx_{i_n}, \mathbf{0}\right]$ and the document vector is zero. The labeling module is then a PLT model denoted by $g : \R^d \rightarrow  \{ 0 , 1 \}^m$ which projects the text representation to the unit vector $g( \mathbf{1}^T  f(\bd))$.


DocTag2Vec is a more sophisticated model, since it makes use of document representation, however it does not make use of the text representation. The text representation module is an extended CBOW model where a document vector is added to each context that is specific to the document. Formally, the tagging module can be written as $f(\bd; \bx_1, \dots, \bx_N)=\left[\bx_{i_1}, \dots , \bx_{i_n}, \bc_i \right]$ where $\bc_i \in \R ^d$ is the representation of document. 



\vspace{\sectionBefore}
\subsection{Probabilistic Label Trees}
\vspace{\sectionAfter}

The general idea of PLTs is relatively simple. 

To introduce \Algo{PLT}s more formally, denote a tree by $T$ and the root of the tree $r(T)$. In general, $T$ can be of any form; here, we consider trees of height $k$ and degree $b$. 
The leaves of $T$ correspond to labels. We denote a set of leaves of a (sub)tree rooted in node $t$ by $L(t)$. %, and the internal nodes of the subtree by $N(t)$. %If $n$ is the root of $T$ then we write $N$. 
%The root is also an internal node, i.e., $t \in N(t)$. 
The parent node of $t$ is denoted by $\pa{t}$, and the set of child nodes by $\Children{t}$. The path from the root $r(T)$ to the $j$\/th leaf is denoted by $\Path{j}$. %and the internal nodes on this path by $\Path{i,N}$.

\Algo{PLT}s use a path from a root to the $j$\/th leaf to estimate posteriors $\eta(\bx, j)$. %, similarly as in probabilistic classifier trees. 
%In other words, we divide the process of estimating $\Pr(y_i \given \bx)$ to $k+1$ stages, each corresponding to a level of the tree $T$. 
With each node $t$ and training instance $\bx$, we associate a label $z_t = \assert{\textstyle \sum_{j \in L(t)} y_j \ge 1}$.
%Recall that $L(t)$ is a set of all leaves of a subtree with the root in the $t$-th node. 
In the leaf nodes, the labels $z_j$, $j \in L$, correspond to the original labels $y_j$.

Consider the leaf node $j$ and the path from the root to this leaf node. Using the chain rule of probability, we can express $\eta(\bx, j)$ in the following way:
%\begin{equation}
%\Pr(y_i = 1 \given \bx) = \prod_{j \in \Path{i}} \Pr(z_j = 1 \given z_{\pa{j}} = 1, \bx)\,,
%\label{eqn:probabilistic_tree}
%\end{equation}
%where for the root node $r(T)$ we have  $\Pr(z_{r(T)} = 1 \given z_{\pa{r(T)}} = 1, \bx) = \Pr(z_{r(T)} = 1 \given \bx)$.
\begin{equation}
\eta(\bx, j) = \prod_{t \in \Path{j}} \eta_T(\bx, t)\,,
\label{eqn:probabilistic_tree}
\end{equation}
where $\eta_T(\bx, t) = \prob(z_t = 1 \given z_{\pa{t}} =1, \bx)$ for all non-root nodes $t$, and $\eta_T(\bx, t) = \prob(z_t = 1 \given \bx)$ if $t$ is the root node. 
The correctness of (\ref{eqn:probabilistic_tree}) follows from the observation that $z_{t} = 1$ implies $z_{\pa{t}} = 1$. A detailed derivation of the chain rule in this setup is given in Appendix~\ref{app:plt}.


The training algorithm for \Algo{PLT}s is given in Algorithm~\ref{alg:pt-learning}.
Let $\cD_n = \{ (\bx_i, \by_{i}) \}_{i=1}^n$ be a training set of multi-label examples.
To learn classifiers in all nodes of a tree $T$, we need to properly filter training examples to estimate $\eta_T(\bx,t)$ (line 5). Moreover, we need to use a learning algorithm $A$ that trains a class probability estimator $\heta_T(\bx,t)$ for each node $t$ in the tree. %a probability estimator trained by such an algorithm in node~$j$. 
The training algorithm returns a set of probability estimation classifiers $\mathcal{Q}$.

The learning time complexity of \Algo{PLT}s can be expressed in terms of the number of nodes in which an original training example $(\bx,\by)$ is used. Since the training example is used in a node $t$ only if $t$ is the root or $z_{\pa{t}} = 1$, this number is upper bounded by $s\cdot b \cdot k+1$, where $b$ and $k$ denote the degree and height of the tree, respectively, and $s$ is the number of positive labels in $\by$. For sparse labels, this value is much lower than $m$. Note that learning can be performed simultaneously for all nodes, and each node classifier can be trained using online methods, such as stochastic gradient descent~\cite{Bottou_2010}.

Interestingly, in the case of sparse features, the space complexity can be significantly reduced as well. Admittedly, the number of models is the highest for binary trees and can be as high as $2m-1$ (notice that the size of a tree with $m$ leaves is upper bounded by $2m-1$). This is twice the number of models in the simplest 1-vs-all approach. Paradoxically, the space complexity can be the lowest at this upper bound. This is because only the sibling nodes need to share the same features, while no other features are needed to build corresponding classifiers. Therefore, only those (few) features needed to describe the sibling labels have to be used in the models. If the space complexity still exceeds the available memory, one can always use feature hashing over all nodes \cite{Weinberger_et_al_2009}.

%\vspace{-2pt}
%
\begin{algorithm}[t]
\caption{\Algo{PLT.Train}$(T, A, \cD_n)$}%Learning of a Probabilistic Label Tree}
\label{alg:pt-learning}
\begin{algorithmic}[1]
%\State \textbf{input:} a label tree $T$, a learning algorithm $A$, and a training set $\mathcal{S}$
%\State \textbf{output:} a set of probability estimation classifiers $\mathcal{Q}$
\State $\mathcal{Q} = \emptyset$
\For{each node $t \in T$} 
\State $\cD' = \emptyset$
\For{$i=1 \to n$}    %each training instance $(\bx, \by) \in \mathcal{S}$}
\If{$t$ is \textbf{root} or $z_{\pa{t}} = 1 $}
\State $z_t = \assert{\sum_{j \in L(t)} y_{ij} \ge 1 }$
\State $\cD' = \cD' \cup (\bx_i, z_t)$ 
\EndIf
\EndFor
\State $\heta_T(\bx,t) = A(\cD')$, $\mathcal{Q} = \mathcal{Q} \cup \heta_T(\bx,t) $  
\EndFor
\State \textbf{return} a set of probability estimation classifiers $\mathcal{Q}$. 
\end{algorithmic}
\end{algorithm} 

Prediction with probabilistic label trees relies on estimating (\ref{eqn:probabilistic_tree}) by traversing the tree from the root to the leaf nodes. However, if the intermediate value of this product in node $t$, denoted by $p_t$, is less than a given threshold $\tau$, then the subtree starting in node $t$ is no longer explored. For the sake of completeness, we shortly describe this procedure (see Algorithm~\ref{alg:pt-prediction}). We start with setting $\hat{\by} = \vec{0}_m$. In order to traverse a tree, we initialize a queue $Q$ to which we add the root node $r_T$ with its posterior estimate $\heta_T(\bx,r(T))$. In the while loop, we iteratively pop a node from $Q$ and compute $p_t$. If $p_t \ge \tau$, we either set $\hat{y}_j = 1$ if $t$ is a leaf, or otherwise add its child nodes to $Q$ with the value $p_t$ multiplied by their posterior estimates $\heta_T(\bx, c)$, $c \in \Children{t}$. If $Q$ is empty, we stop the search and return $\hat{\by}$.
%With properly estimated probabilities, the algorithm will not explore a large part of the tree.

\begin{algorithm}[t]
\caption{\Algo{PLT.Predict}$(\bx, T, \mathcal{Q}, \tau)$}%Prediction with a Probabilistic Label Tree}
\label{alg:pt-prediction}
\begin{algorithmic}[1]
%\State \textbf{input:} a label tree $T$,  a set of probability estimation classifiers $\mathcal{Q}$, a test example $\bx$, a threshold $\tau$
%\State \textbf{input:} a label vector $\hat{\by}$ 
\State $\hat{\by} = \vec{0}_m$, $Q = \emptyset$, $Q.\mathrm{add}(r(T),\heta_T(\bx,r(T)))$ 
\While{$Q \ne \emptyset$}
\State{$(t,p_t) = \mathrm{pop}(Q)$} 
%\State $p_j = p \cdot \heta(j,\bx)$ 
%\State $h(j,\bx) = \sgn(p_j \ge \tau)$
\If{$p_t \ge \tau$} 
\If{$t$ is a leaf node} 
\State $\hat{y}_t = 1$ 
\Else
\For{$c \in \Children{t}$} 
\State $Q.\mathrm{add}(c, p_t \cdot \heta_T(\bx,c))$ 
\EndFor
\EndIf
\EndIf
\EndWhile
\State \textbf{return} $\hat{\by}$. 
\end{algorithmic}
\end{algorithm} 


Traversing a label tree can be much cheaper than querying $m$ independent classifiers, one for each label. If there is only one label exceeding the threshold, \Algo{PLT} ideally needs to call only $bk+1$ classifiers (all classifiers on a path from the root to a leaf plus all classifiers in siblings of the path nodes). Of course, in the worst-case scenario, the entire tree might be explored, but even then, no more than $2m-1$ calls are required (with $m$ leaves, the size of the tree is upper bounded by $2m-1$). In the case of sparse label sets, \Algo{PLT}s can significantly speed up the classification procedure. The expected cost of prediction depends, however, on the tree structure and accuracy of node classifiers.

%Setting thresholds!!!

Note that \Algo{PLT}s can be used with any value as a threshold. Moreover, by considering a separate threshold $\tau_t$ in each node $t$, one can use a different threshold for each label $j$ on posterior estimates $\hat\eta(\bx,j)$. It is enough to set a threshold in the parent node $t$ as follows:
$\tau_t = \min_{j \in \Children{t}} \tau_j$. 
In this way, \Algo{PLT}s  can efficiently obtain SPEs for any $\bkappa$ in \Algo{STO} and \Algo{FTA} (Algorithm~\ref{alg:eum}), and any $\btau$ in \Algo{OFO} (Algorithm~\ref{alg:ofo}). 
%
\Algo{PLT}s can easily be tailored for Precision@K, too. To predict the top labels, it is enough to change $Q$ to a priority queue and stop the prediction procedure after a given number of top labels. %Precision@K can be then efficiently computed. 

Let us finally underline that  \Algo{PLT}s obey strong theoretical guarantees. In  Appendix~\ref{app:plt}, we derive a surrogate regret bound showing that the overall error of \Algo{PLT}s is reduced by improving the node classifiers. For optimal node classifiers, we obtain optimal multi-label classifiers in terms of estimation of posterior probabilities $\eta(\bx,j)$.


\vspace{\sectionBefore}
\subsection{Correction of probability estimates}
\label{sec:online_PLTs}
\vspace{\sectionAfter}

The resulting marginal probabilities of PLT, or in general all node probability models, may not correspond to a valid underlying distribution. For example, it is possible that a parent node will predict a probability of at least 1 positive label in its subtree being greater than a sum of similar probabilities of its children. So, in general the constraints in the $b$-ary tree should be of the following type:
$$p_{pa} \ge p_j\,,~j=1,\ldots, k\,, \quad \mathrm{and} \quad  p_{pa} \le \sum_{j=1}^k p_j\,, $$
where $p_pa$ is the probability in the parent node and $p_j$ is the probability in its$ $j-th child node.
The first constraint is satisfied by the algorithm since $p_j = p_{pa} \times q_j$, where $q_j$ is the probability returned by the estimator in the $j$-th child node. The problem is the second constraint. A possible solution is to do the following. Let 
$$
s = \sum_{j=1}^k p_j\,.
$$
Then, if $s < p_{pa}$ the probabilities $p_j$, for all $j = 1, \ldots, k$, should be corrected according to:
$$
p_j = \frac{p_j \times p_{pa}}{s}\,.
$$


\vspace{\sectionBefore}
\subsection{Generalization of HSM}
\label{sec:online_PLTs}
\vspace{\sectionAfter}

Hierarchical softmax (HSM) and conditional probability estimation trees (CPET) are only for multi-class problems. 
FastText also uses HSM. To deal with multi-label problems it applies a very simple heuristic. It randomly picks one of the labels for a given training instance and treats the problem as multi-class. During prediction, it returns multi-class distribution and the most probable label only. It can be easily extended to return the top $k$ labels, as the tree is traversed using a priority queue search. We will later show that this approach is not consistent in general. Before that we will show that PLTs described above are no-regret generalization of HSM. First of all let us observe that PLTs over a multi-class distribution gets the same model as HSM. In sake of simplicity, we will consider a binary tree in the following.  
Take the probabilities of sibling nodes, $l$ and $r$ in PLT:
$$
\prob(z_l = 1 \given z_{\pa{l}} =1, \bx) \quad \prob(z_r = 1 \given z_{\pa{r}} =1, \bx)
$$
Since $\pa{l} = \pa{r}$ (as $l$ and $r$ sibling nodes, they need to have the same parent), the condition in the both probabilities above is the same. Moreover, since $z_{\pa{r}} =1$ there is at least one active label in the subtree rooted in the parent of $r$ and $l$. Since we deal with multi-class classification, there is exactly one such label. This further implies that $z_l + z_r = 1$, and finally: 
$$
\prob(z_l = 1 \given z_{\pa{l}} =1, \bx) + \prob(z_r = 1 \given z_{\pa{r}} =1, \bx) = 1 
$$
In other words, the model in node $l$ is the complement of the model in node $r$, i.e., both sibling nodes can be collapsed to one node with one model with the left and right decision. This reasoning nicely generalizes furthers to $b$-ary trees,

\begin{figure}[h]
		\begin{tikzpicture}[scale = 0.75,every node/.style={scale=0.75},
		regnode/.style={circle,draw,minimum width=1.5ex,inner sep=0pt},
		leaf/.style={circle,fill=black,draw,minimum width=1.5ex,inner sep=0pt},
		pleaf/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt},
		pnode/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt},
		rootnode/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt},
		activerootnode/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt,line width=1pt},
		activenode/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt,line width=1pt},
		activepleaf/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt,line width=1pt},
		level/.style={sibling distance=18em/#1, level distance=9ex}
		]
		\node (z) [rootnode] {\thickmuskip=-1.5mu $\prob(y_1\lor y_2 \lor y_3 \lor y_4 \given\bx)$}
		child {node (a) [pnode] {\thickmuskip=-1.5mu $\prob(\overbrace{y_1 \lor y_2}^{z_l}\given \overbrace{y_1 \lor y_2 \lor y_3  \lor y_4}^{z_{\mathrm{pa}(l)}}=1, \bx)$} 
			%child {node [label=below:{$y_1$}] (b) [pleaf,fill=lightgray] {\thickmuskip=-1.5mu $\prob(y_1\given y_1 \lor y_2=1, \bx)$} edge from parent node[above left]{}}
			%child {node [label=below:{$y_2$}] (g) [pleaf] {\thickmuskip=-1.5mu $\prob(y_2\given y_1 \lor y_2 = 1, \bx)$} edge from parent node[above right]{}}
			edge from parent node[above left]{}
		}
		child {node (j) [pnode] {$\prob(\overbrace{y_3 \lor y_4}^{z_r} \given \overbrace{y_1 \lor y_2 \lor y_3 \lor y_4}^{z_{\mathrm{pa}(r)}} = 1, \bx)$}
		%	child {node [label=below:{$y_3$}] (k) [pleaf] {\thickmuskip=-1.5mu $\prob(y_3\given y_3 \lor y_4 = 1, \bx)$} edge from parent node[above left]{}}
		%	child {node [label=below:{$y_4$}] (l) [pleaf] {\thickmuskip=-1.5mu $\prob(y_4\given y_3 \lor y_4 = 1,  \bx)$}
		%		{
		%			child [grow=right] {node (s) {} edge from parent[draw=none]
		%				child [grow=up] {node (t) {} edge from parent[draw=none]
		%					child [grow=up] {node (u) {} edge from parent[draw=none]}
		%				}
		%			}
		%		}
		%		edge from parent node[above right]{}
		%	}
			edge from parent node[above right]{}
		};
		\end{tikzpicture}
\caption{PLTs generalizes HSM}
\end{figure}


It can be easily shown that PLTs are consistent for any multi-label probability distribution $\prob$ (including the multi-class distributions).  

This is not a case of the pick-one-label heuristic used for HSM. This heuristics maps the multi-label distribution to a multi-class distribution in the following way:\footnote{We use the same notation for the probability of the $i$-th label in the multi-class distribution as the marginal probability of the multi-label distribution, since the multi-class distribution can be also coded by vectors $\by$, but with a constraint that one and only one label is active}
$$
\eta'(\bx, j) = \prob'(y_j = 1 \given \bx) = \sum_{\by \in \calY} y_j \frac{\prob(\by \given \bx)}{\sum_{j'=1}^m y_{j'}}
$$
This is specific marginalization with respect to $y_j$ in which we divide the conditional probability of  $\prob(\by \given \bx)$ by the number of ones in $\by$. To see that the pick-one-label does not lead to optimal solution let us consider a following conditional distribution for some $\bx$ given in the table below.
\begin{center}
\begin{tabular}{c c}
\toprule
labels $\by$ & probability $\prob(\by \given \bx)$ \\
\midrule
$\{1\}$ & 0.15 \\
$\{2\}$ & 0.1 \\
$\{1, 2\}$ & 0.25 \\
$\{3\}$ & 0.3 \\
$\{4\}$ & 0.2 \\
\bottomrule
\end{tabular}
\end{center}
The optimal top 1 prediction for this example is obviously label $1$, since the marginal probabilities are $\eta(\bx,1) = 0.4, \eta(\bx,2) = 0.35,  \eta(\bx,3) = 0.3, \eta(\bx,4) =0.2$. However, the pick-one-label heuristic will transform the original distribution to the following one: $\eta'(\bx,1) = 0.275, \eta'(\bx,2) = 0.225,  \eta'(\bx,3) = 0.3, \eta'(\bx,4) =0.2$. The predicted top label will be then label $3$, giving the regret of 0.1. 

%splits the probability of labels $\{1,2\}$ to half probabilities of $\{1\}$ and $\{2\}$ yielding the following marginal probabilities: $\eta(\bx,1) = 0.275, \eta(\bx,2) = 0.225,  \eta(\bx,3) = 0.3, \eta(\bx,4) =0.2$. 

It is obvious that the heuristic will change the marginal probabilities of labels (unless the distribution is multi-class), therefore in general, it will lead to inconsistent solution in terms of logistic loss. 
Interestingly, if the labels are conditionally independent, i.e.,:
$$
\prob(\by \given \bx) = \prod_{j=1}^m \prob(y_i \given \bx)\,,
$$
the optimal solution in terms of precision@k does not change. To show this let us observe the following.
Let $y_i$ and $y_j$ be so that $\prob(y_i = 1 \given \bx) \ge \prob(y_j = 1 \given \bx) $. Then in the summation over of $\by$s, we are interested in four different subsets of $\calY$: 
$$
S_{i,j}^{u,w}  =  \{\by\in \calY: y_i = u \land y_j = w\} \quad u,w \in \{0,1\} \,.
$$
During mapping any $\by \in S^{0,0}_{i,j}$ does not play any role. For each $\by \in S^{1,1}_{i,j}$, the value of $y_t \frac{\prob(\by \given \bx)}{\sum_{t'=1}^m y_{t'}}$ is the same for both $y_i$ and $y_j$. Now, let $\by' \in S^{1,0}_{i,j}$ and $\by'' \in S^{0,1}_{i,j}$ be the same on all elements except the $i$-th and the $j$-th one. Then, because   $\prob(y_i = 1 \given \bx) \ge \prob(y_j = 1 \given \bx) $, we will have $\prob(\by' \given \bx) \ge \prob(\by'' \given \bx)$. Therefore, after mapping we will get $\eta'(\bx,i) \ge \eta'(\bx, j)$. 


%0.1 0.2
%0    0    0.72
%0    1    0.18
%1    0    0.08
%1    1    0.02
%
%
%0.09 0.019
%
%0.1 0.5
%0    0    0.45
%0    1    0.45
%1    0    0.05
%1    1    0.05

%0.075 0.475



\vspace{\sectionBefore}
\section{PLTs for document tagging}
\label{sec:online_PLTs}
\vspace{\sectionAfter}



\vspace{\sectionBefore}
\subsection{Online training of PLTs}
\label{sec:online_PLTs}
\vspace{\sectionAfter}


\vspace{\sectionBefore}
\subsection{PLTs with sparse input}
\label{sec:sparse_input}
\vspace{\sectionAfter}


\vspace{\sectionBefore}
\subsection{PLTs with word embeddings}
\label{sec:sparse_input}
\vspace{\sectionAfter}


\vspace{\sectionBefore}
\subsection{PLTs with recurrent neural network}
\label{sec:sparse_input}
\vspace{\sectionAfter}


\vspace{\sectionBefore}
\subsection{PLTs with LSTM}
\label{sec:sparse_input}
\vspace{\sectionAfter}


\vspace{\sectionBefore}
\section{Empirical results}
\label{sec:empirical_results}
\vspace{\sectionAfter}





\vspace{\sectionBefore}
\section{Formal setup of doc tagging}
\label{sec:formal}
\vspace{\sectionAfter}


Input is given in form of document and set of tags pairs. Documents consist of words that are taken from a dictionary which is a finite indexed set $\calW = \{ w_1, \dots , w_N \}$. A document is a sequence of words  $\bd_i = (w_{i,1}, \dots, w_{i,n_i})\in \calW^{n_i}$. Its tag set is represented  by a binary vector $\by_i = (y_{i,1}, \ldots, y_{i,m}) \in \{ 0 , 1 \}^m$ where $m$ is the number of possible tags. A set of documents is denoted by $\cD = \{ (\bw_i, \by_{i}) \}_{i=1}^n$. For sake of simplicity, we assume that $n_i = n$ for every $1\le i \le n$.

Our approach consists of two modules: tagging and text representation modules. The text representation module $f : \calW^{n} \rightarrow \R^{d \times n+1}$ is a (not necessarily) parametric function whose input is a document and its output is the vector presentation of both words and docs. The dimension of representation space is $d$. In the simplest case, this text representation module can be the bag-of-words presentation, in which case $d=N$, and the doc vector (n+1\/th column of the image space of function $f$) is an all zero vector. A more sophisticated text presentation module is the word2vec methodology which assign a vector to each word whose dimension is typically $\le300$, and the doc presentation is empty. 

The tagging module's input is the output of the text presentation module, that is $g: \R^{d \times n+1} \rightarrow  \{ 0 , 1 \}^m$. 

In case of Deep PLT, the text representation module simply assigns a vector $\bx_{i_j} \in \R^d$ to each word of the given document $\bd_i$ where $i_j$ is the index of $j$\/th word of the document $i$, i.e. $f(\bd; \bx_1, \dots, \bx_N) = f(\bd) = \left[\bx_{i_1}, \dots , \bx_{i_n}, \mathbf{0}\right]$ and the document vector is zero. The labeling module is then a PLT model denoted by $g : \R^d \rightarrow  \{ 0 , 1 \}^m$ which projects the text representation to the unit vector $g( \mathbf{1}^T  f(\bd))$.


DocTag2Vec is a more sophisticated model, since it makes use of document representation, however it does not make use of the text representation. The text representation module is an extended CBOW model where a document vector is added to each context that is specific to the document. Formally, the tagging module can be written as $f(\bd; \bx_1, \dots, \bx_N)=\left[\bx_{i_1}, \dots , \bx_{i_n}, \bc_i \right]$ where $\bc_i \in \R ^d$ is the representation of document. 



\vspace{\sectionBefore}
\subsection{Probabilistic Label Trees}
\vspace{\sectionAfter}

To introduce \Algo{PLT}s more formally, denote a tree by $T$ and the root of the tree $r(T)$. In general, $T$ can be of any form; here, we consider trees of height $k$ and degree $b$. 
The leaves of $T$ correspond to labels. We denote a set of leaves of a (sub)tree rooted in node $t$ by $L(t)$. %, and the internal nodes of the subtree by $N(t)$. %If $n$ is the root of $T$ then we write $N$. 
%The root is also an internal node, i.e., $t \in N(t)$. 
The parent node of $t$ is denoted by $\pa{t}$, and the set of child nodes by $\Children{t}$. The path from the root $r(T)$ to the $j$\/th leaf is denoted by $\Path{j}$. %and the internal nodes on this path by $\Path{i,N}$.

\Algo{PLT}s use a path from a root to the $j$\/th leaf to estimate posteriors $\eta(\bx, j)$. %, similarly as in probabilistic classifier trees. 
%In other words, we divide the process of estimating $\Pr(y_i \given \bx)$ to $k+1$ stages, each corresponding to a level of the tree $T$. 
With each node $t$ and training instance $\bx$, we associate a label $z_t = \assert{\textstyle \sum_{j \in L(t)} y_j \ge 1}$.
%Recall that $L(t)$ is a set of all leaves of a subtree with the root in the $t$-th node. 
In the leaf nodes, the labels $z_j$, $j \in L$, correspond to the original labels $y_j$.

Consider the leaf node $j$ and the path from the root to this leaf node. Using the chain rule of probability, we can express $\eta(\bx, j)$ in the following way:
%\begin{equation}
%\Pr(y_i = 1 \given \bx) = \prod_{j \in \Path{i}} \Pr(z_j = 1 \given z_{\pa{j}} = 1, \bx)\,,
%\label{eqn:probabilistic_tree}
%\end{equation}
%where for the root node $r(T)$ we have  $\Pr(z_{r(T)} = 1 \given z_{\pa{r(T)}} = 1, \bx) = \Pr(z_{r(T)} = 1 \given \bx)$.
\begin{equation}
\eta(\bx, j) = \prod_{t \in \Path{j}} \eta_T(\bx, t)\,,
\label{eqn:probabilistic_tree}
\end{equation}
where $\eta_T(\bx, t) = \prob(z_t = 1 \given z_{\pa{t}} =1, \bx)$ for all non-root nodes $t$, and $\eta_T(\bx, t) = \prob(z_t = 1 \given \bx)$ if $t$ is the root node. 
The correctness of (\ref{eqn:probabilistic_tree}) follows from the observation that $z_{t} = 1$ implies $z_{\pa{t}} = 1$. A detailed derivation of the chain rule in this setup is given in Appendix~\ref{app:plt}.


The training algorithm for \Algo{PLT}s is given in Algorithm~\ref{alg:pt-learning}.
Let $\cD_n = \{ (\bx_i, \by_{i}) \}_{i=1}^n$ be a training set of multi-label examples.
To learn classifiers in all nodes of a tree $T$, we need to properly filter training examples to estimate $\eta_T(\bx,t)$ (line 5). Moreover, we need to use a learning algorithm $A$ that trains a class probability estimator $\heta_T(\bx,t)$ for each node $t$ in the tree. %a probability estimator trained by such an algorithm in node~$j$. 
The training algorithm returns a set of probability estimation classifiers $\mathcal{Q}$.

The learning time complexity of \Algo{PLT}s can be expressed in terms of the number of nodes in which an original training example $(\bx,\by)$ is used. Since the training example is used in a node $t$ only if $t$ is the root or $z_{\pa{t}} = 1$, this number is upper bounded by $s\cdot b \cdot k+1$, where $b$ and $k$ denote the degree and height of the tree, respectively, and $s$ is the number of positive labels in $\by$. For sparse labels, this value is much lower than $m$. Note that learning can be performed simultaneously for all nodes, and each node classifier can be trained using online methods, such as stochastic gradient descent~\cite{Bottou_2010}.

Interestingly, in the case of sparse features, the space complexity can be significantly reduced as well. Admittedly, the number of models is the highest for binary trees and can be as high as $2m-1$ (notice that the size of a tree with $m$ leaves is upper bounded by $2m-1$). This is twice the number of models in the simplest 1-vs-all approach. Paradoxically, the space complexity can be the lowest at this upper bound. This is because only the sibling nodes need to share the same features, while no other features are needed to build corresponding classifiers. Therefore, only those (few) features needed to describe the sibling labels have to be used in the models. If the space complexity still exceeds the available memory, one can always use feature hashing over all nodes \cite{Weinberger_et_al_2009}.

%\vspace{-2pt}
%
\begin{algorithm}[t]
\caption{\Algo{PLT.Train}$(T, A, \cD_n)$}%Learning of a Probabilistic Label Tree}
\label{alg:pt-learning}
\begin{algorithmic}[1]
%\State \textbf{input:} a label tree $T$, a learning algorithm $A$, and a training set $\mathcal{S}$
%\State \textbf{output:} a set of probability estimation classifiers $\mathcal{Q}$
\State $\mathcal{Q} = \emptyset$
\For{each node $t \in T$} 
\State $\cD' = \emptyset$
\For{$i=1 \to n$}    %each training instance $(\bx, \by) \in \mathcal{S}$}
\If{$t$ is \textbf{root} or $z_{\pa{t}} = 1 $}
\State $z_t = \assert{\sum_{j \in L(t)} y_{ij} \ge 1 }$
\State $\cD' = \cD' \cup (\bx_i, z_t)$ 
\EndIf
\EndFor
\State $\heta_T(\bx,t) = A(\cD')$, $\mathcal{Q} = \mathcal{Q} \cup \heta_T(\bx,t) $  
\EndFor
\State \textbf{return} a set of probability estimation classifiers $\mathcal{Q}$. 
\end{algorithmic}
\end{algorithm} 

Prediction with probabilistic label trees relies on estimating (\ref{eqn:probabilistic_tree}) by traversing the tree from the root to the leaf nodes. However, if the intermediate value of this product in node $t$, denoted by $p_t$, is less than a given threshold $\tau$, then the subtree starting in node $t$ is no longer explored. For the sake of completeness, we shortly describe this procedure (see Algorithm~\ref{alg:pt-prediction}). We start with setting $\hat{\by} = \vec{0}_m$. In order to traverse a tree, we initialize a queue $Q$ to which we add the root node $r_T$ with its posterior estimate $\heta_T(\bx,r(T))$. In the while loop, we iteratively pop a node from $Q$ and compute $p_t$. If $p_t \ge \tau$, we either set $\hat{y}_j = 1$ if $t$ is a leaf, or otherwise add its child nodes to $Q$ with the value $p_t$ multiplied by their posterior estimates $\heta_T(\bx, c)$, $c \in \Children{t}$. If $Q$ is empty, we stop the search and return $\hat{\by}$.
%With properly estimated probabilities, the algorithm will not explore a large part of the tree.

\begin{algorithm}[t]
\caption{\Algo{PLT.Predict}$(\bx, T, \mathcal{Q}, \tau)$}%Prediction with a Probabilistic Label Tree}
\label{alg:pt-prediction}
\begin{algorithmic}[1]
%\State \textbf{input:} a label tree $T$,  a set of probability estimation classifiers $\mathcal{Q}$, a test example $\bx$, a threshold $\tau$
%\State \textbf{input:} a label vector $\hat{\by}$ 
\State $\hat{\by} = \vec{0}_m$, $Q = \emptyset$, $Q.\mathrm{add}(r(T),\heta_T(\bx,r(T)))$ 
\While{$Q \ne \emptyset$}
\State{$(t,p_t) = \mathrm{pop}(Q)$} 
%\State $p_j = p \cdot \heta(j,\bx)$ 
%\State $h(j,\bx) = \sgn(p_j \ge \tau)$
\If{$p_t \ge \tau$} 
\If{$t$ is a leaf node} 
\State $\hat{y}_t = 1$ 
\Else
\For{$c \in \Children{t}$} 
\State $Q.\mathrm{add}(c, p_t \cdot \heta_T(\bx,c))$ 
\EndFor
\EndIf
\EndIf
\EndWhile
\State \textbf{return} $\hat{\by}$. 
\end{algorithmic}
\end{algorithm} 


Traversing a label tree can be much cheaper than querying $m$ independent classifiers, one for each label. If there is only one label exceeding the threshold, \Algo{PLT} ideally needs to call only $bk+1$ classifiers (all classifiers on a path from the root to a leaf plus all classifiers in siblings of the path nodes). Of course, in the worst-case scenario, the entire tree might be explored, but even then, no more than $2m-1$ calls are required (with $m$ leaves, the size of the tree is upper bounded by $2m-1$). In the case of sparse label sets, \Algo{PLT}s can significantly speed up the classification procedure. The expected cost of prediction depends, however, on the tree structure and accuracy of node classifiers.

%Setting thresholds!!!

Note that \Algo{PLT}s can be used with any value as a threshold. Moreover, by considering a separate threshold $\tau_t$ in each node $t$, one can use a different threshold for each label $j$ on posterior estimates $\hat\eta(\bx,j)$. It is enough to set a threshold in the parent node $t$ as follows:
$\tau_t = \min_{j \in \Children{t}} \tau_j$. 
In this way, \Algo{PLT}s  can efficiently obtain SPEs for any $\bkappa$ in \Algo{STO} and \Algo{FTA} (Algorithm~\ref{alg:eum}), and any $\btau$ in \Algo{OFO} (Algorithm~\ref{alg:ofo}). 
%
\Algo{PLT}s can easily be tailored for Precision@K, too. To predict the top labels, it is enough to change $Q$ to a priority queue and stop the prediction procedure after a given number of top labels. %Precision@K can be then efficiently computed. 

Let us finally underline that  \Algo{PLT}s obey strong theoretical guarantees. In  Appendix~\ref{app:plt}, we derive a surrogate regret bound showing that the overall error of \Algo{PLT}s is reduced by improving the node classifiers. For optimal node classifiers, we obtain optimal multi-label classifiers in terms of estimation of posterior probabilities $\eta(\bx,j)$.



\vspace{\sectionBefore}
\subsection{Related work}
\vspace{\sectionAfter}

\Algo{PLT}s share similarities with conditional probability estimation trees~\cite{Beygelzimer_et_al_2009b} and probabilistic classifier chains~\cite{Dembczynski_et_al_2010c}, while being suited to estimate marginal posterior probabilities $\eta(\bx,j)$. They are also similar to Homer~\cite{Tsoumakas_et_al_2008}, which transforms training examples in the same way but does not admit a probabilistic interpretation. Let us also remark that a similar concept is known in neural networks and natural language processing under the name of hierarchical softmax~classifiers \cite{Morin_Bengio_2005}; however, it has not been used in a multi-label scenario.

In a nutshell, \Algo{PLT}s are based on the label tree approach~\cite{Beygelzimer_et_al_2009a,Bengio_et_al_2010,Deng_et_al_2011}, in which each leaf node corresponds to one label. Classification of a test example relies on a sequence of decisions made by node classifiers, leading the test example from the root to the leaves of the tree. Since \Algo{PLT}s are designed for multi-label classification, each internal node classifier decides whether or not to continue the path by moving to the child nodes. This is different from typical left/right decisions made in tree-based classifiers.  Moreover, a leaf node classifier needs to make a final decision regarding the prediction of a label associated to this leaf. \Algo{PLT}s use a class probability estimator in each node of the tree, such that an estimate of the posterior probability of a label associated with a leaf is given by the product of the probability estimates on the path from the root to that leaf. Prediction then relies on traversing the tree from the root to the leaf nodes. Whenever the intermediate value of the product of probabilities in an internal node is less than a given threshold, the subtree below this node is not explored anymore. This pruning strategy leads to a very efficient classification procedure. %We introduce \Algo{PLT} in more detail below. 


\newpage



\bibliography{xmlc_references}
\bibliographystyle{icml2016}

\appendix

\onecolumn

\section{Regret for Prec@k}
Top-k labels with respect to the true marginals
\[
M_k(\bx) = \left\{ i\in [m] : \# \left\{ j \in [m] : \eta(\bx, j) \ge \eta(\bx, i)  \right\} \le k\right\}
\]

Top-k labels with respect to the estimated marginals
\[
\hat{M}_k(\bx) = \left\{ i\in [m] : \# \left\{ j \in [m] : \hat{\eta}(\bx, j) \ge \hat{\eta}(\bx, i)  \right\} \le k\right\}
\]


Expected regret for Prec@k conditioned on $\bx$ 
\[
\mbox{Reg}_{P@k} (h, \bx) = \sum_{i \in M_k(\bx)} \eta (\bx ,i ) - \sum_{j \in [m]} h(\bx, j) \eta(\bx , j )
\]

We shall focus on multi-label classifiers in the following form:
\[
h(\bx,i) =h_{k, \hat{\eta}}(\bx,i) = \assert{\textstyle i \in \hat{M}_k(\bx) }
\]

In this case the Prec@k regret is $[0,1]$.

The error for marginal estimates is denoted by $m_i = \vert \eta (\bx ,i ) - \hat{\eta} (\bx ,i )\vert$. With this we can upper bound 
\begin{align}
\mbox{Reg}_{P@k} (h, \bx) 
  & = \sum_{i \in M_k(\bx)} \eta (\bx ,i ) - \sum_{j \in [m]} h(\bx, j) \eta(\bx , j ) \notag \\
  & \le \sum_{i \in M_k(\bx)} \hat{\eta} (\bx ,i ) + m_i - \sum_{j \in [m]} h(\bx, j) \eta(\bx , j ) \notag \\ 
  & \le \sum_{i \in \hat{M}_k(\bx)} (\hat{\eta} (\bx ,i ) + m_i )- \sum_{j \in \hat{M}_k(\bx)}  \eta(\bx , j ) \notag \\
  & = \sum_{i \in \hat{M}_k(\bx)} 2m_i \notag \\
  & \le 2k\max_{i \in [m]} m_i
\end{align}

\section{Probabilistic label trees}
\label{app:plt}

An example of a probabilistic label tree (\Algo{PLT}) for 4 labels $(y_1,y_2,y_3,y_4)$ is given in Fig.~\ref{fig:plt}. To estimate posterior probability $\eta(\bx, j) = \prob(y_j = 1 \given \bx)$, \Algo{PLT} uses a path from a root to the $j$-th leaf.
In each node $t$, we associate with a training instance $(\bx,\by)$ a label $z_t$ such that:
$$
z_t = \assert{\textstyle \sum_{j \in L(t)} y_j \ge 1} \quad \textrm{(or equivalently $z_t = \bigvee_{j \in L(t)} y_j$)}
$$
Recall that $L(t)$ is a set of all leaves of a subtree with the root in the $t$-th node. In leaf nodes the labels $z_j$, $j \in L$, correspond to original labels $y_j$.
%
\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[scale = 0.85,every node/.style={scale=0.85},
		regnode/.style={circle,draw,minimum width=1.5ex,inner sep=0pt},
		leaf/.style={circle,fill=black,draw,minimum width=1.5ex,inner sep=0pt},
		pleaf/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt},
		pnode/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt},
		rootnode/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt},
		activerootnode/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt,line width=1pt},
		activenode/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt,line width=1pt},
		activepleaf/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt,line width=1pt},
		level/.style={sibling distance=16em/#1, level distance=12ex}
		]
		\node (z) [rootnode,fill=lightgray] {\thickmuskip=-1.5mu $\prob(y_1\lor y_2 \lor y_3 \lor y_4 \given\bx)$}
		child {node (a) [pnode,fill=lightgray] {\thickmuskip=-1.5mu $\prob(\overbrace{y_1 \lor y_2}^{z_t}\given \overbrace{y_1 \lor y_2 \lor y_3  \lor y_4}^{z_{\mathrm{pa}(t)}}=1, \bx)$} 
			child {node [label=below:{ \scriptsize \thickmuskip=-1.5mu $y_1$}] (b) [pleaf,fill=lightgray] {\thickmuskip=-1.5mu $\prob(y_1\given y_1 \lor y_2=1, \bx)$} edge from parent node[above left]{}}
			child {node [label=below:{\scriptsize \thickmuskip=0mu $y_2$}] (g) [pleaf] {\thickmuskip=-1.5mu $\prob(y_2\given y_1 \lor y_2 = 1, \bx)$} edge from parent node[above right]{}}
			edge from parent node[above left]{}
		}
		child {node (j) [pnode] {$\prob(y_3 \lor y_4 \given y_1 \lor y_2 \lor y_3 \lor y_4 = 1, \bx)$}
			child {node [label=below:{ \scriptsize \thickmuskip=0mu $y_3$}] (k) [pleaf] {\thickmuskip=-1.5mu $\prob(y_3\given y_3 \lor y_4 = 1, \bx)$} edge from parent node[above left]{}}
			child {node [label=below:{ \scriptsize \thickmuskip=0mu $y_4$}] (l) [pleaf] {\thickmuskip=-1.5mu $\prob(y_4\given y_3 \lor y_4 = 1,  \bx)$}
				{
					child [grow=right] {node (s) {} edge from parent[draw=none]
						child [grow=up] {node (t) {} edge from parent[draw=none]
							child [grow=up] {node (u) {} edge from parent[draw=none]}
						}
					}
				}
				edge from parent node[above right]{}
			}
			edge from parent node[above right]{}
		};
		\end{tikzpicture}
\end{center}
\caption{An example of a probabilistic label tree for 4 labels $(y_1,y_2,y_3,y_4)$.}
\label{fig:plt}
\end{figure}
%

Consider the leaf node $j$ and the path from the root to this leaf node. Using the chain rule of probability, we can express $\eta(\bx, j)$ in the following way:
\begin{equation}
\eta(\bx, j) = \prod_{t \in \Path{j}} \eta_T(\bx, t)\,,
\label{eqn:probabilistic_tree_appendix}
\end{equation}
where $\eta_T(\bx, t) = \prob(z_t = 1 \given z_{\pa{t}} =1, \bx)$, for all non-root nodes $t$, and $\eta(\bx, t) = \prob(z_t = 1 \given \bx)$, if $t$ is the root node (denoted by $r(T)$). 

To see the correctness of (\ref{eqn:probabilistic_tree_appendix}) notice that $z_{t} = 1$ implies $z_{\pa{t}} = 1$. So, for non-root nodes $t$ and $\pa{t}$ we have:
\begin{eqnarray*}
\eta_T(\bx,t) \eta_T(\bx, \pa{t}) & = &  \prob(z_t = 1 \given z_{\pa{t}} =1, \bx)\prob(z_{\pa{t}} = 1 \given z_{\pa{\pa{t}}} =1, \bx)\\
& = & \frac{\prob(z_t = 1 , z_{\pa{t}} =1, \bx)}{\prob(z_{\pa{t}} =1, \bx)} \frac{\prob(z_{\pa{t}} = 1, z_{\pa{\pa{t}}} =1, \bx)}{\prob(z_{\pa{\pa{t}}} =1, \bx)} \\
& = & \frac{\prob(z_t = 1, \bx)}{\prob(z_{\pa{t}} =1, \bx)} \frac{\prob(z_{\pa{t}} = 1, \bx)}{\prob(z_{\pa{\pa{t}}} =1, \bx)} \\
& = & \frac{\prob(z_t = 1, \bx)}{\prob(z_{\pa{\pa{t}}} =1, \bx)} \,.
\end{eqnarray*}
In words, the probability associated with the parent node $\pa{t}$ cancels out and we can express $\eta_T(\bx,t) \eta_T(\bx, \pa{t})$ as the product of probabilities associated only with node $t$ and its grandparent node $\pa{\pa{t}}$. 
%
Applying the above rule consecutively to $\prod_{t \in \Path{j}} \eta_T(\bx, t)$ and recalling that for the root note $\eta_T(\bx, r(T)) = \prob(z_{r(T)} = 1 \given \bx)$, we finally get $\eta(\bx, j)$. 

Below we show that \Algo{PLT}s posses strong theoretical guarantees. We derive a relation between  estimation error minimized by the node classifiers and estimation error of posterior probabilities $\eta(\bx,j)$. This relation states that we can upperbound the latter error by the former. This also implies that for optimal node classifiers we get optimal multi-label classifier in terms of estimation of posterior probabilities.


We are interested in bounding the estimation error of posterior probabilities of labels at point $\bx$
$$
\ell(\eta(\bx),\heta(\bx)) = \frac{1}{m} \sum_{j=1}^m |\eta(\bx, j) - \heta(\bx, j)| \,,
$$
in terms of an estimation error of node classifiers
$$
\ell(\eta_T(\bx, t), \heta_T(\bx, t)) = |\eta_T(\bx, t) - \heta_T(\bx, t)  | \,.
$$

Expressing $\eta(\bx, j)$  and $\heta(\bx, j)$ by (\ref{eqn:probabilistic_tree_appendix}) and applying Lemma~2 from~\cite{Beygelzimer_et_al_2009b}, we get:
\begin{equation}
\left | \eta(\bx, j) - \heta(\bx, j) \right | \le \sum_{t \in \Path{j}} \left | \eta_T(\bx, t) - \heta_T(\bx, t) \right | \,.
\label{eqn:estimation_bound}
\end{equation}

Equation (\ref{eqn:estimation_bound}) already shows that minimization of the estimation error by node classifiers improves the overall performance of \Algo{PLT}s. We can show, however, even a more general result concerning surrogate regret bounds by referring to the theory of  strongly proper composite losses~\cite{Agarwal_2014}. 

Assume that a node classifier has a form of a real-valued function $f_t$. Moreover, there exists a strictly increasing (and therefore invertible) link function $\psi: [0,1] \rightarrow \mathbb{R}$ such that $f_t(\bx) = \psi(\heta_T(\bx,t))$. Recall that the regret of $f_t$ in terms of a loss function $\ell$ at point $\bx$ is defined as:
$$
\reg_{\ell}(f_t \given \bx) = L_{\ell}(f_t \given \bx) - L_{\ell}^*(\bx) \,,
$$
where $L_{\ell}(f_t \given \bx)$ is the expected loss  at  point $\bx$:
$$
L_{\ell}(f \given \bx) = \mathbb{E}_{y_j\given\bx} \left [ \ell  (y_j, f_t(\bx)) \right ] \,,
$$
and  $L_{\ell}^*(\bx)$ is the minimum expected loss at point $\bx$.

If a node classifier is trained by a learning algorithm that minimizes a strongly proper composite loss, e.g.,  squared, exponential, or logistic loss, like in our implementation (see in Appendix \ref{sec:training_of_node_classifiers}), then the bound (\ref{eqn:estimation_bound}) can be expressed in terms of the regret of this loss function: 
$$
\left | \eta_T(\bx, t) - \psi^{-1}(f_t)  \right | \le \sqrt{ \frac{2}{\lambda}} \sqrt{\reg_\ell(f_t \given \bx)}
$$
where $\lambda$ is a strong properness constant specific for a given loss function (for more detail, see~\cite{Agarwal_2014}). By putting the above inequality into~(\ref{eqn:estimation_bound}), we get
$$
\left | \eta(\bx, j) - \heta(\bx, j) \right | \le \! \sum_{t \in \Path{j}} \! \left | \eta_T(\bx, t) - \heta_T(\bx, t) \right | = \!  \sum_{t \in \Path{j}}  \! \left | \eta_T(\bx, t) - \psi^{-1}(f_t)  \right | \le  \! \sum_{t \in \Path{j}}  \! \sqrt{ \frac{2}{\lambda}} \sqrt{\reg_\ell(f_t \given \bx)}
$$ 


%Consider the label-wise KL-divergence between true posterior probabilities $\eta(\bx, j)$ and their estimates $\heta(\bx, j)$:
%$$
%\KLD(\eta(\bx) || \heta(\bx)) = \frac{1}{m}\sum_{j=1}^m \KLD(\eta(\bx,j) || \heta(\bx,j)) = \frac{1}{m}\sum_{j=1}^m \left ( \eta(\bx,j) \log \frac{\eta(\bx,j)}{\heta(\bx,j)} + (1 -\eta(\bx,j)) \log \frac{1- \eta(\bx,j)}{1 - \heta(\bx,j)}\right ) 
%$$
%Let us remark that $\KLD(\eta(\bx,j) || \heta(\bx,j))$  is equivalent to the conditional logistic regret defined as:
%$$
%\reglog(\heta(\bx,j) \given \bx) = L_{\log}(\heta(\bx,j) \given \bx) - L_{\log}^*(\bx) \,,
%$$
%where $L_{\log}(\heta(\bx,j) \given \bx)$ is the expected logistic risk at  point $\bx$:
%$$
%L_{\log}(\heta(\bx,j) \given \bx) = \mathbb{E}_{y_j\given\bx} \ell_{\log} (y_j, \heta(\bx,j)) = \mathbb{E}_{y_j\given\bx} \left ( y_j \log \heta(\bx,j) + (1 - y_j) \log (1 - \heta(\bx,j)) \right ) \,.
%$$
%and  $L_{\log}^*(\bx)$ is the minimum logistic risk at point $\bx$ achieved by $\heta(\bx,j) = \eta(\bx,j)$. %as says a standard result on the theory of proper losses (see, e.g., \citep{Reid_Williamson_2010}).
%
%Because of (\ref{eqn:probabilistic_tree_appendix}), we can rewrite the above to:
%$$
%\KLD(\eta || \heta) = \frac{1}{m}\sum_{j=1}^m \left (  \prod_{t \in \Path{j}} \eta_T(\bx, t)\log \frac{ \prod_{t \in \Path{j}} \eta_T(\bx, t)}{\ \prod_{t \in \Path{j}} \heta_T(\bx, t)} + (1 - \prod_{t \in \Path{j}} \eta_T(\bx, t)) \log \frac{1-  \prod_{t \in \Path{j}} \eta_T(\bx, t)}{1 -  \prod_{t \in \Path{j}} \heta_T(\bx, t)}\right ) 
%$$

\section{Training of node classifiers}
\label{sec:training_of_node_classifiers}

In each node $t$ we trained a linear classifier $f_t(\bx) = \bw \cdot \bx$, where $\bx = (1, x_1, \ldots, x_p)$.  To this end we used a variant of stochastic gradient descent to minimize logistic loss. Albeit successfully used in large scale learning, the optimization of empirical loss using stochastic gradient descent is a particularly challenging task when the number of features and labels is large. The step function should ensure a quick convergence in order to reduce the number of required training epochs. Furthermore, it should support sparse updates of the weights (i.e., only weights for non-zero features should be updated to ensure fast training time). % and auxiliary data structures. 
\citet{Duchi_Singer_2009} propose a two phase gradient step:
\begin{align*}
	\label{eq:fobos}
	\bm{w}_{t+\frac12} &= \bm{w}_t - \gamma_t \bm{g}_t\\
	\bm{w}_{t+1} &= \argmin_{\bm{w}}\left\{ \frac12 \norm[\big]{ \bm{w} - \bm{w}_{t+\frac12}}^2
	+ \lambda \gamma_{t} r(\bm{w})\right\}
\end{align*}
where $\bm{w}_t$ is the weight vector at time step $t$, $r(\bm{w})$ is a
regularization function, $\lambda$ is regularization parameter, and $\gamma_t$ is an adaptive step size, and $\bm{g}_t$ is the gradient vector at $\bm{x}_t$ of logistic loss applied to the linear model $f_t$. 

For stochastic gradient descent with $L_2^2$ regularization,
the step function reduces to
\begin{equation*}\label{eq:fobosl2}
	\bm{w}_{t+1} = \frac{\bm{w}_t - \gamma_t \bm{g}_t}{1 + \lambda\gamma_{t}}
\end{equation*}

By using  
$$
\Pi_t = \prod_{i=1}^t (1 + \lambda\gamma_{t}) 
\quad \mathrm{and} \quad \tilde{\bw}_t = \Pi_{t} \bw_t \,,
$$
we can rewrite the step function to the following form:
$$
\tilde{\bw}_{t+1}  = \tilde{\bw}_t - \Pi_{t}  \gamma_t \bm{g}_t
$$
Thanks to this transformation, we are able to make sparse updates by storing only the current value of $\Pi_t$ (one value for each node classifier). This is because the $i$-th component of $\tilde{\bw}$ does not change when $x_i$ is zero. More formally, 
$$
\tilde{w}_{i,t+1}  = \tilde{w}_{i,t} \mathrm{, if~} g_{i,t} = 0 \,.
$$
During prediction or computation of gradient $\bm{g}_t$, we use:
$$
\bw_t = \frac{\tilde{\bw}_{t}}{\Pi_{t}} \,.
$$

In our implementation we adapt the step size $\gamma_t$ as suggested in \citep{Bottou_2012}:
$$
\gamma_t = \frac{\gamma}{1 + \lambda \gamma t} \,,
$$
where $\gamma$ is an initial parameter.


\section{Tuning of hyperparameters}
\label{sec:hyper}


A \Algo{PLT} has only one global hyperparameter which is the degree of the tree denoted by $b$. The other hyperparameters are associated with the node classifiers. To tune the stochastic gradient descent described above we varied values of $\gamma$, $\lambda$, and the number of epochs. 
%
All hyperparameters were tuned by using the open-source hyperparameter optimizer \Algo{SMAC}~\cite{Hutter_et_al_2011} with a wide range of parameters, which is reported in Table \ref{tab:hyppar}. 
%We also tuned the number of epochs which is the number of times we run through the entire dataset. 
The validation process was carried out by using a $80/20$ split of the training data for every dataset we used.

\vspace{\tableBefore}
\begin{table}[ht!]
\caption{The hyperparameters of the \Algo{PLT} method and their ranges used in hyperparameter optimization,}
\label{tab:hyppar}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Hyperparameter & Validation range \\%& RCV1 & AmazonCat & Wiki10 & Delicious-200K & WikiLSHTC & Amazon \\ 
\hline
$b$ & $\{2,\dots,256\}$ \\% &  $16$          & $16$      & $32$   & $2$     & &       \\ 
$\lambda$ &  $[10 - 0.000001]$ \\%& $10^{-4}$ & $10^{-4}$   & $10^{-5}$& $10^{-5}$ & &       \\ 
$\gamma$ &  $[10 - 0.000001]$ \\%& $10^{-5}$  & $10^{-5}$   & $10^{-5}$& $10^{-6}$ & &       \\ 
Num. of epochs &  $\{ 5, \dots , 30\} $ \\
\hline
\end{tabular}
\end{center}
\end{table}
\vspace{\tableAfter}




\section{F-scores by tuning the input parameters $\ba$ and $\bb$ of \Algo{OFO} algorithms}
\label{sec:OFO_a_b}

In our experimental study described in Section \ref{sec:experiments}, we did not tune the input parameter $\ba$ of the \Algo{OFO} algorithm but set all of its components to $1$. We carried out experiments for assessing the impact of the input parameter $\ba$ on the performance of \Algo{OFO}. Its optimal value was selected from the set $C = \{10000,1000,200,100,50,20,10,7,5,4,3,2\}$ based on the same validation process like in case of $\bb$ and we took into account the fact that $a_i /b_i$ should be in range $\left (\hat\pi_j / (\hat\pi_j + 1), 0.5 \right ]$, as it was pointed out in (\ref{eqn:range}). The macro F-scores computed for the test and validation set are shown in Table \ref{tab:ofo_a_b} along with the validated values of $a_i$ and $b_i$. For sake of readability, we repeat here the scores achieved by \Algo{STO} and \Algo{FTA} reported earlier in Table \ref{tab:initthreshold}. The macro F-scores achieved by \Algo{OFO} are slightly better thanks to the additional degree of freedom, and thus, the \Algo{OFO} algorithm outperforms \Algo{FTA} and \Algo{STO} algorithm on almost every datasets except the Amazon dataset in which case the \Algo{OFO} and \Algo{FTA} algorithms are tied.

%\begin{table}[ht!] 
%	\caption{The test macro F-scores obtained by validating both input parameters $a$ and $b$} 
%	\label{tab:ofo_a_b}
%	\begin{center}
%		\begin{small}
%			%\vspace{-0.1cm}
%			\begin{tabular}{l@{\hskip 3pt}| c@{\hskip 4pt} c@{\hskip 4pt} c@{\hskip 4pt} | c@{\hskip 4pt} c@{\hskip 4pt} c@{\hskip 4pt} }
%				\toprule
%Algorithm & Dataset & $a_i$ & $b_i$ & Valid. F-score & Test F-score & Test F-score  \\
%\midrule
%PLT & RCV1 & $300$ & $20000$ & $22.20$ & $22.00$ \\
%PLT & AmazonCat & $700$ & $5000$ & $33.37$ & $35.30$ \\
%PLT & wiki10 & $100$ & $200$ & $55.27$ & $30.28$ \\
%PLT & Delicious-200K & $100$ & $200$ & $34.88$ & $11.20$ \\
%PLT & WikiLSHTC & $100$ & $200$ & $39.94$ & $14.00$ \\
%PLT & Amazon & $100$ & $200$ & $54.84$ & $51.28$ \\
%\midrule
%FastXML & RCV1 & $1000$ & $100000$ & $19.84$ & $19.28$\\
%FastXML & AmazonCat & $10000$ & $100000$ & $50.21$ & $41.48$\\
%FastXML & wiki10 & $5000$ & $100000$ & $54.72$ & $29.91$\\
%FastXML & Delicious-200K & $100$ & $500$ & $35.02$ & $11.20$\\
%FastXML & WikiLSHTC & $5000$ & $100000$ & $45.78$ & $21.38$\\
%FastXML & Amazon & $10000$ & $100000$ & $53.91$ & $52.86$\\
%				\bottomrule
%			\end{tabular}
%		\end{small}
%	\end{center}
%\end{table}

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother

\begin{table}[ht!] 
	\caption{The test macro F-scores obtained by validating both input parameters $a$ and $b$. The numbers in bold indicate the best score achieved on each dataset.} 
	\label{tab:ofo_a_b}
	\begin{center}
		\begin{small}
			%\vspace{-0.1cm}
			\begin{tabular}{l@{\hskip 3pt}| c@{\hskip 4pt} c@{\hskip 4pt} c@{\hskip 4pt} | c@{\hskip 4pt} c@{\hskip 4pt}  | c@{\hskip 4pt} | c@{\hskip 4pt}  }
				\toprule
		  &         &       &       &  \multicolumn{2}{c|}{\Algo{OFO}} & \Algo{FTA} & \Algo{STO} \\           	
Algorithm & Dataset & $a_i$ & $b_i$ & Valid. F-score & Test F-score & Test F-score & Test F-score  \\
\midrule
\Algo{PLT} & RCV1 & 300 & 20000 & 22.20 & { \bf 22.00 } & 20.41 & 21.16 \\
\Algo{PLT} & AmazonCat & 700 & 5000 & 33.37 & 35.30 & 34.83 & 31.64 \\
\Algo{PLT} & wiki10 & 100 & 200 & 55.27 & { \bf 30.28 } & 29.98 & 24.02 \\
\Algo{PLT} & Delicious-200K & 100 & 200 & 34.88 & {\bf 11.20} & 11.12 & 10.96 \\
\Algo{PLT} & WikiLSHTC & 100 & 200 & 39.94 & 14.00 & 12.31 & 16.22 \\
\Algo{PLT} & Amazon & 100 & 200 & 54.84 & 51.28 & 51.77 & 46.94 \\
\midrule
\Algo{FastXML} & RCV1 & 1000 & 100000 & 19.84 & 19.28 & 17.04 & 19.58 \\
\Algo{FastXML} & AmazonCat & 10000 & 100000 & 50.21 & { \bf 41.48 } & 41.07 & 37.28 \\
\Algo{FastXML} & wiki10 & 5000 & 100000 & 54.72 & 29.91 & 29.88 & 28.26 \\
\Algo{FastXML} & Delicious-200K & 100 & 500 & 35.02 & { \bf 11.20 } & 11.18 & 10.83 \\
\Algo{FastXML} & WikiLSHTC & 5000 & 100000 & 45.78 & { \bf 21.38 } & 21.24 & 20.41 \\
\Algo{FastXML} & Amazon & 10000 & 100000 & 53.91 & {\bf 52.86} & {\bf 52.86} & 47.53 \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
\end{table}


%\section{Average number of predicted positives on the validation and test data}
%\label{sec:predpos}
%
%
%We compared the computational complexity of various methods in terms of the number of predicted positives which is a valid indicator of testing time. The average number of predicted positives with the validated input parameters are reported in Table \ref{tab:avgpos}, but here we additionally plot the number of predicted positives for various input parameters of the threshold tuning algorithm. The average number of predicted positives are shown in Figures \ref{fig:plt_predpos} and \ref{fig:fastxml_predpos} for \Algo{PLT} and \Algo{FastXML}, respectively. Based on the results we can see that the average number of positive labels are an order of magnitude smaller than the number of labels for every case both on the test and validation dataset, even for small input parameter values. Therefore we could compute, and thus validate the  input parameters of threshold tuning algorithms over a wide range of values in an efficient way. Note that running \Algo{STO} with $\kappa_i = 0$ all $\bigO ( nm )$ posteriors would have been computed which is not feasible in XMLC.
%
%\begin{figure*}[ht!]
%\centerline{
%  \parbox{2.0\textwidth}{
%    \center    
%    \includegraphics[width=0.3\textwidth]{\figures/RCV1_predpos}    
%    \includegraphics[width=0.3\textwidth]{\figures/AmazonCat_predpos}    
%    \includegraphics[width=0.3\textwidth]{\figures/wiki10_predpos}    \\
%	\includegraphics[width=0.3\textwidth]{\figures/Delicious-200K_predpos}    
%	\includegraphics[width=0.3\textwidth]{\figures/WikiLSHTC_predpos}    
%    \includegraphics[width=0.3\textwidth]{\figures/Amazon_predpos}        
%  }
%}
%\caption{Number of predicted positives for test and validation data by using \Algo{STO}, \Algo{FTA} and \Algo{OFO} algorithms along with \Algo{PLT} on various datasets. 
%\label{fig:plt_predpos}}
%\vspace{-0.3cm}
%\end{figure*}
%
%
%\begin{figure*}[ht!]
%\centerline{
%  \parbox{2.0\textwidth}{
%    \center    
%    \includegraphics[width=0.3\textwidth]{\figures/RCV1_predpos_fastxml}    
%    \includegraphics[width=0.3\textwidth]{\figures/AmazonCat_predpos_fastxml}    
%    \includegraphics[width=0.3\textwidth]{\figures/wiki10_predpos_fastxml} \\
%    \includegraphics[width=0.3\textwidth]{\figures/Delicious-200K_predpos_fastxml} 
%    \includegraphics[width=0.3\textwidth]{\figures/WikiLSHTC_predpos_fastxml}    
%    \includegraphics[width=0.3\textwidth]{\figures/Amazon_predpos_fastxml}    
%  }
%}
%\caption{Number of predicted positives for test and validation data by using \Algo{STO}, \Algo{FTA} and \Algo{OFO} algorithms along with \Algo{FastXML} on various datasets.
%\label{fig:fastxml_predpos}}
%\vspace{-0.3cm}
%\end{figure*}


%\section{Treating missing labels}
%
%In multi-label classification to split the data into training and test set is an important issue, because the concurrence of the labels it is hard to achieve that each label will be presented both in the test and the training set. 















\end{document} 



