\documentclass{article}

\usepackage{fullpage,amssymb,amsthm,amsmath}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{array,graphicx}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}

\usepackage{algpseudocode}
\usepackage{algorithm}


\newcommand{\Gm}{L_{\max}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\E}{\mathbb{E}}

\newcommand{\cD}{\mathcal{D}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\path}{\text{Path}}
\newcommand{\Path}[1]{\mathrm{Path}(#1)}
\newcommand{\pa}[1]{\mathrm{pa}(#1)}
\newcommand{\leaves}{d}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\vec{y}}
\newcommand{\bY}{\vec{Y}}
\newcommand{\prob}{\mathbf{P}} 

\newcommand{\given}{\, | \,}

\newcommand{\Algo}[1]{\textsc{#1}}

\begin{document}

\section{Model}

The data $\cD = \{ (\bx_{i},\by_{i})\}_{i=1,\dots,n}$ consists of feature vector $\bx_i \in \R^d$ and label vector $\by_i\in \{ 0,1\}^m$ tuples. The labels can be written in a matrix form $\bY = [y_{i,j}]$ whose $j$\/th column is denoted by $\bar{\by}_{j}$.

We will work with the set of rooted trees with $m$ leaves which is denoted by $\cT$. For sake of simplicity, we assume that the arity of the trees is $k=2$. The leaves of a tree $T$ is denoted by $\ell_1, \dots , \ell_m$. The path from the root to the leaf $\ell_i$ is denoted by $\path (i )$. 

There is given a set of labeling functions $\Lambda$ which contains binary functions in the form of $\lambda: V(T) \mapsto \{0,1\}$. For a labeling function $\lambda$ that is compatible with $(\bx,\by)$, it holds that $\lambda(\ell_j) = y_{j}$ for all $j\in [m]$. The set of labeling functions that is compatible with $\bx$ is denoted by $\Lambda_{\bx}$. We shall concisely write 
%$\lambda_{\bx_i}=\lambda_i$ and 
$\Lambda_{\bx_i} = \Lambda_{i}$. In words, $\Lambda_{\bx}$ contains every binary assignment to the inner nodes which matches with the label vector $\by$ of $\bx$ on the leaves.

In addition, we are given a function class $\cH : \R^d \mapsto [0,1]$ which set contains probabilistic classifiers, i.e. logistic regressors. We assign a classifier from $\cH$ to each node of the tree $T$. We shall index this (possibly multi-) subset of classifiers by the element of $V(T)$ as $H = \{ h_{v} \in \cH : v\in V(T) \}$. The set of all multi-set from $\cH$ with $k$ element is denoted by  $\cH_k$.

The Generalized Probabilistic Label Trees (GPLT) model is defined as follows: the conditional probability estimate for an instance $\bx$ with respect to the model with tree $T$, labeling function $\lambda \in \Lambda$ and classifier set $H$,  is
\[
\eta( \bx, j ) = \prod_{v \in \path(\ell_j)} \left( \lambda(v) h_v (\bx) + (1-\lambda(v) ) (1-h_v (\bx)) \right)
\]
Note that this expression depends on the tree structure via the path from the root to the leaf $\ell_j$. If this does not lead any confusion, we do not indicate that $\eta$ depends on $T,\lambda,H$, that is, $\eta_{T,H,\lambda}( \bx_i, j ) = \eta( \bx_i, j ) $.

The likelihood of $\cD$ can be written in the form of
\[
\cL ( T, H, \{ \lambda_i, \dots, \lambda_n\} ) = \prod_{i=1}^{n} \prod_{j=1}^m \eta_{T,H,\lambda_i} ( \bx_i , j )^{y_{i,j}} (1-\eta_{T,H,\lambda_i} ( \bx_i , j ))^{(1-y_{i,j})}
\]
when we assume independence between the labels. There are several multi-label likelihood function which aims at capturing the dependence of labels~\citep{ZhangS12}. 

Each internal node $t$ of a tree has three associated random variables: $z_{l(t)}$, $z_{r(t)}$, and $z_{lr(t)}$. The first one, $z_{l(t)}$, says that $\bx$ has positive label(s) in the left subtree only, the second one, $z_{r(t)}$, says that $\bx$ has positive label(s) in the right subtree, and the last one, $z_{rl(t)}$, says that there are positive labels in both subtrees. Let us assume without loss of generality that $\Path{\ell_j}$ consists of nodes being the left children of the parent nodes. Then, the marginal probability $\eta_{T,H,\lambda_i} ( \bx_i , j )$ can be defined by:
\begin{equation}
\eta(\bx, j) = \prod_{t \in \Path{\ell_j}}  \left ( \prob(z_{l(t)} = 1 \given z_{\pa{t}} =1, \bx) + \prob(z_{lr(t)} = 1 \given z_{\pa{t}} =1, \bx) \right ) \,.
\label{eqn:probabilistic_tree}
\end{equation}

Let us now define the maximum likelihood of the tree. The main problem is to express the probability of $(1-\eta_{T,H,\lambda_i} ( \bx_i , j ))$, i.e., that the $j$-th label is not positive. By using the tree, we can write this in the following way:
$$
(1-\eta_{T,H,\lambda_i} ( \bx_i , j )) = \sum_{t \in \Path{\pa{\ell_j}}}   \prob(z_{r(t)} = 1 | \bx)  \prod_{t' \in \Path{\pa{t}}} \left ( \prob(z_{l(t')} = 1 | \bx) + \prob(z_{lr(t')} = 1 \given \bx) \right )  \,.
$$

TODO: Maximum likelihood problem, inference problem

\subsection{Relation to Ancestral Maximum Likelihood}

Assume a set of simple classifier $\cH_{C} = \{h_p\in \cH : p\in [0,1] \}$ where $h_p(\bx) = p$ for all $\bx \in \R^d$. Thus using s set of simple classifier, a constant value will assigned to $0$ that is denoted by $p_v$.

Likelihood with simple classifiers can be written as
\begin{align}
\cL ( T, H, \{ \lambda_i, \dots, \lambda_n\} ) 
  & = \prod_{i=1}^{n} \prod_{j=1}^m \left[ \prod_{v \in \path(\ell_j)} \left( \lambda_i (v) p_v + (1-\lambda_i (v) ) (1-p_v) \right)\right]^{y_{i,j}} \times \notag \\ & ~~~~~~~~~~~~~~~~~~~~ \left[ 1-\prod_{v \in \path(\ell_j)} \left( \lambda_i (v) p_v + (1-\lambda_i (v) ) (1-p_v) \right) \right]^{1-y_{i,j}}  \notag
%   & = \prod_{j=1}^m \prod_{v \in \path(\ell_j)}\prod_{i\in I_i^+}  \left[ \lambda_i (v) p_v + (1-\lambda_i (v) ) (1-p_v) \right] \prod_{i\in I_i^-} \left[ 1-\left( \lambda_i (v) p_v + (1-\lambda_i (v) ) (1-p_v) \right) \right] \notag \\
%   & = \prod_{j=1}^m \prod_{v \in \path(\ell_j)} p_{v}^{s_v} (1-p_v)^{n^+-s_v} \notag \\
%   & = \prod_{v \in V(T)} p_{v}^{\leaves (v) s_v} (1-p_v)^{\leaves (v)(n-s_v)} \notag
\end{align}
where $s_v = \sum_{i=1}^n \lambda_i (v)$ and $\leaves (v)$ is the number of leaves below $v$ in the tree $T$, $\leaves( \ell_i ) = 1$ and $I_j^{+} = \{ i \in [n]: y_{i,j}=1 \}$ is index of positive labels of $(\bx_i, \by_i)$, and $I_i^{-}$ is the index set of negative labels. $n^+_i = \# I_i^{+}$

The likelihood function bears a resemblance to the objective of Ancestral Maximum Likelihood (\Algo{AML})~\citep{Alon2010}, except the $d(v)$ in the exponent.

TODO: Exact reduction to \Algo{AML}, then to show that inference is hard, i.e. find a label and labeling with fixed T and H that maximizes the likelihood

\section{Positive leaf-to-root labeling}

We restrict the set of labeling function in this section. We only consider such labeling function $\lambda$ that is compatible with $(\bx, \by )$ and assigns $1$ to each $v\in \path(\ell_i)$ whenever $y_i$, and $0$ otherwise. In this case, the labeling function is determined by the tree $T$ and labels uniquely. 

The likelihood with $H$ and PLR labeling can be written as
\begin{align}
\cL ( T, H) 
  & = \prod_{i=1}^{n} \prod_{j=1}^m \prod_{v \in \path(\ell_j)} \left( \lambda_i (v) p_v + (1-\lambda_i (v) ) (1-p_v) \right) \notag \\
  & = \prod_{i=1}^{n} \prod_{j\in I_i^{+}}^m \prod_{v \in \path(\ell_j)} \left( \lambda_i (v) p_v + (1-\lambda_i (v) ) (1-p_v) \right) \notag \\
\end{align}

\section{Sub-optimality of \Algo{LOM} criterion}

The \Algo{LOM} criterion is an evaluation criterion for PLTs with positive leaf-to-root labeling~\citep{Choromanska_Langford_2015}.

\bibliography{xmlc_references,treeb}
\bibliographystyle{icml2018}


\end{document}